{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJ2X_5sG7Qox"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install --quiet \"torchmetrics >=1.0,<1.5\" \"seaborn\" \"numpy <3.0\" \"torchvision\" \"tensorboard\" \"torch >=1.8.1,<2.5\" \"pytorch-lightning >=2.0,<2.5\" \"matplotlib\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "from copy import deepcopy\n",
        "from urllib.error import HTTPError\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib_inline.backend_inline\n",
        "import pytorch_lightning as pl\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import STL10\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "plt.set_cmap(\"cividis\")\n",
        "%matplotlib inline\n",
        "matplotlib_inline.backend_inline.set_matplotlib_formats(\"svg\", \"pdf\")  # For export\n",
        "matplotlib.rcParams[\"lines.linewidth\"] = 2.0\n",
        "sns.set()\n",
        "\n",
        "# Import tensorboard\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "kp_z2uj77UXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Data Preparation\n",
        "\n",
        "# Path configurations\n",
        "DATASET_PATH = \"data/\"\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "# Data augmentations for SimCLR\n",
        "contrast_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomResizedCrop(size=96),\n",
        "    transforms.RandomApply([\n",
        "        transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1)\n",
        "    ], p=0.8),\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    transforms.GaussianBlur(kernel_size=9),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Datasets for contrastive learning\n",
        "unlabeled_data = STL10(\n",
        "    root=DATASET_PATH,\n",
        "    split=\"unlabeled\",\n",
        "    download=True,\n",
        "    transform=contrast_transforms\n",
        ")\n",
        "train_data_contrast = STL10(\n",
        "    root=DATASET_PATH,\n",
        "    split=\"train\",\n",
        "    download=True,\n",
        "    transform=contrast_transforms\n",
        ")\n"
      ],
      "metadata": {
        "id": "4nrL0bJt7UZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## SimCLR model definition\n",
        "\n",
        "class SimCLR(pl.LightningModule):\n",
        "    def __init__(self, hidden_dim=128, lr=1e-3, temperature=0.07, weight_decay=1e-4, max_epochs=100):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        # INTERNAL NOTES\n",
        "        # 1. In the future, try with other backbones (could be Resnet50, could be some EfficientNet)\n",
        "        # 2. Since SimCLR learns representations directly from the data, I'm not using the pre-trained weights by now\n",
        "        # to avoid the bias learned from ImageNet-like datasets. However, this should also be tested.\n",
        "        # Tip: for larger datasets, pretrained=False should work better (because of what I've just exposed)\n",
        "        # while for small datasets, the pretrained weights might provide a performance boost\n",
        "        # Define ResNet backbone\n",
        "        self.convnet = torchvision.models.resnet18(pretrained=False)\n",
        "        self.convnet.fc = nn.Sequential(\n",
        "            nn.Linear(self.convnet.fc.in_features, 4 * hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4 * hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.hparams.max_epochs, eta_min=self.hparams.lr / 50)\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "    def info_nce_loss(self, batch):\n",
        "        imgs, _ = batch\n",
        "        imgs = torch.cat(imgs, dim=0)\n",
        "        feats = self.convnet(imgs)\n",
        "        cos_sim = F.cosine_similarity(feats[:, None, :], feats[None, :, :], dim=-1)\n",
        "        self_mask = torch.eye(cos_sim.shape[0], device=cos_sim.device, dtype=torch.bool)\n",
        "        cos_sim.masked_fill_(self_mask, -9e15)\n",
        "        pos_mask = self_mask.roll(shifts=cos_sim.shape[0] // 2, dims=0)\n",
        "        cos_sim = cos_sim / self.hparams.temperature\n",
        "        nll = -cos_sim[pos_mask] + torch.logsumexp(cos_sim, dim=-1)\n",
        "        return nll.mean()\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss = self.info_nce_loss(batch)\n",
        "        self.log(\"train_loss\", loss)\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "efngwUkA7Uce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Train SimCLR Model\n",
        "\n",
        "def train_simclr(batch_size=256, max_epochs=100, **kwargs):\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=max_epochs,\n",
        "        accelerator=\"gpu\",\n",
        "        devices=1,\n",
        "        callbacks=[\n",
        "            ModelCheckpoint(save_weights_only=True, monitor=\"train_loss\", mode=\"min\"),\n",
        "            LearningRateMonitor(logging_interval=\"epoch\"),\n",
        "        ]\n",
        "    )\n",
        "    train_loader = DataLoader(unlabeled_data, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS)\n",
        "    model = SimCLR(max_epochs=max_epochs, **kwargs)\n",
        "    trainer.fit(model, train_loader)\n",
        "    return model\n",
        "\n",
        "# Train the SimCLR model\n",
        "simclr_model = train_simclr(hidden_dim=128, lr=1e-3, temperature=0.07, weight_decay=1e-4, max_epochs=100)\n"
      ],
      "metadata": {
        "id": "HrJVtnDv7Ue-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Save the model\n",
        "\n",
        "torch.save(simclr_model.state_dict(), \"simclr_model.pth\")"
      ],
      "metadata": {
        "id": "KE0lbR907Uhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Define MLP Classifier\n",
        "\n",
        "class MLPClassifier(pl.LightningModule):\n",
        "    def __init__(self, feature_dim, num_classes, hidden_dim=256, lr=1e-3, weight_decay=1e-4, max_epochs=100):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        # Define the MLP architecture\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(feature_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.hparams.max_epochs, eta_min=self.hparams.lr / 50)\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "    def _calculate_loss(self, batch, mode=\"train\"):\n",
        "        feats, labels = batch\n",
        "        preds = self.model(feats)\n",
        "        loss = F.cross_entropy(preds, labels)\n",
        "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
        "\n",
        "        # Log loss and accuracy\n",
        "        self.log(f\"{mode}_loss\", loss, prog_bar=True)\n",
        "        self.log(f\"{mode}_acc\", acc, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        return self._calculate_loss(batch, mode=\"train\")\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        self._calculate_loss(batch, mode=\"val\")\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        self._calculate_loss(batch, mode=\"test\")"
      ],
      "metadata": {
        "id": "9gpbqr057Ush"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Train the MLP Classifier\n",
        "\n",
        "def train_mlp(batch_size, train_feats_data, test_feats_data, max_epochs=100, **kwargs):\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=max_epochs,\n",
        "        accelerator=\"gpu\",\n",
        "        devices=1,\n",
        "        callbacks=[\n",
        "            pl.callbacks.ModelCheckpoint(save_weights_only=True, monitor=\"val_acc\", mode=\"max\"),\n",
        "            pl.callbacks.LearningRateMonitor(\"epoch\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # Data loaders\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_feats_data, batch_size=batch_size, shuffle=True, drop_last=False, num_workers=4\n",
        "    )\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_feats_data, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=4\n",
        "    )\n",
        "\n",
        "    # Train the MLP\n",
        "    model = MLPClassifier(max_epochs=max_epochs, **kwargs)\n",
        "    trainer.fit(model, train_loader, test_loader)\n",
        "\n",
        "    # Test the model\n",
        "    test_result = trainer.test(model, test_loader, verbose=False)\n",
        "    print(f\"Test accuracy: {test_result[0]['test_acc'] * 100:.2f}%\")\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "4tPYqz6j7UvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mcNzmQHd7Uxr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}